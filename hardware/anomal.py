"""anomal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PVA4pwRJLR69Ib9TTXkOHJkPBkK2e7J
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv(
    '/content/recording_20240924_233805_2400.0MHz_2500.0MHz.csv',
    sep=',',
    header=None,
)

# Переименование первого столбца в 'Date'
df.rename(columns={0: 'Date'}, inplace=True)

# Переименование второго столбца в 'Time'
df.rename(columns={1: 'Time'}, inplace=True)

# Переименование третьего столбца в 'Lower Frequency (GHz)'
df.rename(columns={2: 'Lower Frequency (GHz)'}, inplace=True)

# Переименование четвертого столбца в 'Upper Frequency (GHz)'
df.rename(columns={3: 'Upper Frequency (GHz)'}, inplace=True)

# Переименование пятого столбца в 'Bin Width (GHz)'
df.rename(columns={4: 'Bin Width (GHz)'}, inplace=True)

# Переименование шестого столбца в 'Number of Samples'
df.rename(columns={5: 'Number of Samples'}, inplace=True)

# Переименование седьмого столбца в 'Signal Level (dB)'
df.rename(columns={6: '1'}, inplace=True)

df = df.drop(index=0)
df = df.drop("Bin Width (GHz)", axis = 1)
df = df.drop("Number of Samples", axis = 1)
df = df.drop("Lower Frequency (GHz)", axis = 1)
df = df.drop("Upper Frequency (GHz)", axis = 1)
df["Date"]= df ["Date"]+ df ["Time"]
df = df.drop("Time", axis = 1)
df = df.drop("Date", axis = 1)

from sklearn.preprocessing import MinMaxScaler

# todo
def preprocessing(df):
    data = df.copy()

    scaler = MinMaxScaler()
    data.columns = data.columns.astype(float)
    data = scaler.fit_transform(data)

    return data

data = preprocessing(df=df)

from sklearn.preprocessing import StandardScaler

data = pd.DataFrame (data)
# Преобразуем имена колонок в строки
data.columns = data.columns.astype(str)

# Инициализация скэйлера
StSc = StandardScaler()

# Обучение скэйлера на тренировочной выборке (строки с 0 до 5000)
StSc.fit(data.iloc[:5000])

# Применение скэйлера на всех данных
# Преобразование тренировочной выборки
train_sc = StSc.transform(data.iloc[:5000])

# Преобразование валидационной выборки (строки с 5000)
val_sc = StSc.transform(data.iloc[5000:])

# Преобразование всей выборки
data_sc = StSc.transform(data)

from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from sklearn.metrics import mean_absolute_error, mean_squared_error
# from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import EarlyStopping

from itertools import product

def Random(seed_value):
    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value
    import os
    os.environ['PYTHONHASHSEED'] = str(seed_value)

    # 2. Set `python` built-in pseudo-random generator at a fixed value
    import random
    random.seed(seed_value)

    # 3. Set `numpy` pseudo-random generator at a fixed value
    import numpy as np
    np.random.seed(seed_value)

    # 4. Set `tensorflow` pseudo-random generator at a fixed value
    import tensorflow as tf
    tf.random.set_seed(seed_value)

# Функция для обучения конкретной архитектуры модели
def arch(param, data):
    """
    Обучение конкретной архитектуры

    Parameters
    ----------
    param : list

    data : np.array
    """
    Random(0)
    input_dots = Input((5,))

    x = Dense(param[0])(input_dots)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Dense(param[1])(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    bottleneck = Dense(param[2], activation='linear')(x)

    x = Dense(param[1])(bottleneck)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    x = Dense(param[0])(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)

    out = Dense(5, activation='linear')(x)

    model = Model(input_dots, out)
    model.compile(optimizer=Adam(param[3]), loss='mae', metrics=["mse"])

#     early_stopping = EarlyStopping(patience=3, verbose=0)
    model.fit(data, data,
                validation_split=0.2,
                epochs=10,
                batch_size=param[4],
                verbose=0,
                shuffle=True,
#                 callbacks=[early_stopping]
               )
    return model

# Выбор сетки параметров для обучения и тестирования
n1=[6, 5]
n2=[4, 3]
n3=[2, 1]
lr=[0.05, 0.01]
batch_size=[32, 64]

parameters = product(n1, n2, n3, lr, batch_size)
parameters_list = list(parameters)

from tqdm.notebook import tqdm

# "Перебор моделей"
errors = []
for params in tqdm(parameters_list):

    model = arch(params, train_sc)
    train_pred = model.predict(train_sc, batch_size=params[4])
    val_pred = model.predict(val_sc, batch_size=params[4])

    train_error = mean_absolute_error(train_sc, train_pred)
    val_error = mean_absolute_error(val_sc, val_pred)

    errors.append(list(params)+[train_error, val_error])

# Сортировка ошибки
df_errors = pd.DataFrame(errors,
                         columns=['neurons 1st layer',
                                  'neurons 2nd layer',
                                  'neurons 3rd layer',
                                  'learning rate',
                                  'batch size',
                                  'mae train',
                                  'mae val'])
df_errors.sort_values('mae val').head()

best_params = parameters_list[df_errors.sort_values('mae val').index[0]]

model = arch(best_params, train_sc) # train+val стоит использовать, если распределения выборок совпадают
model.summary()

# #прогноз и ошибка лучшей модели на обучающей выборке
train_pred = model.predict(train_sc, batch_size=30)
print('train mae: ', mean_absolute_error(train_sc,train_pred))

# #прогноз и ошибка лучшей модели на тестовой выборке
test_pred = model.predict(data_sc, batch_size=30)
print('test mae: ', mean_absolute_error(data_sc,test_pred))

"""Построение индекса технического состояния (ИТС) на обученной модели"""

train_residuals = train_sc - model.predict(train_sc)
val_residuals = val_sc - model.predict(val_sc)

UCL = pd.DataFrame(val_residuals).abs().sum(axis=1).quantile(0.95)

# можем повысить контрольный предел из-за деградации оборудования для снижения кол-ва ложных тревог
# UCL = 2 * UCL

# print(f'Значение контрольного предела (UCL) = {UCL.round(2)}')
# можем но не будем

test_residuals = data_sc - model.predict(data_sc)

# сигнал
pd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1)
# сглаженный сигнал
pd.DataFrame(test_residuals, index=data.index).abs().sum(axis=1).rolling(11).median()

date_df = pd.read_csv(
    '/content/recording_20240924_233805_2400.0MHz_2500.0MHz.csv',
    sep=',',
    header=None, usecols=[0,1]
).assign(Index=list(range(len(date_df))))

import pandas as pd

# Предполагаем, что test_residuals - это ваш исходный массив остатков
# и UCL - это вычисленная верхняя контрольная граница

# Создаем DataFrame из остатков
residuals_df = pd.DataFrame(test_residuals, index=data.index)

# Вычисляем абсолютные значения и сумму по строкам
signal_levels = residuals_df.abs().sum(axis=1)

# Создаем новый DataFrame с аномальными значениями
anomalies_df = signal_levels[signal_levels > UCL]

# Если нужно, можно сохранить индекс (время) аномальных значений
anomalies_df = anomalies_df.reset_index()

# Переименуем столбцы для удобства
anomalies_df.columns = ['Index', 'Anomaly_Value']

# Объединение датасетов по столбцу
merged_df = pd.merge(date_df, anomalies_df, on='Index')
print(merged_df)